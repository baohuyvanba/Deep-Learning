{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"./attachments/Example.png\" alt=\"\" width=\"600\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, W1_initial, W2_initial, b1 = None, b2 = None):\n",
    "        self.W1 = W1_initial.copy()\n",
    "        self.W2 = W2_initial.copy()\n",
    "\n",
    "        self.b1 = b1 if b1 is not None else np.zeros((self.W1.shape[0], 1))\n",
    "        self.b2 = b2 if b2 is not None else np.zeros((self.W2.shape[0], 1))\n",
    "\n",
    "        self.a0 = None\n",
    "        self.z1 = None\n",
    "        self.a1 = None\n",
    "        self.z2 = None\n",
    "        self.a2 = None\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(a):\n",
    "        return a*(1 - a)\n",
    "    \n",
    "    def forward_pass(self, a0):\n",
    "        print(\"--- Forward Pass ---\")\n",
    "        self.a0 = a0\n",
    "        print(f\"Input a0: {self.a0}\\n\")\n",
    "\n",
    "        # Layer 1\n",
    "        self.z1 = self.W1 @ self.a0 + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        print(f\"z1 = W1 @ a0: \\n{self.z1}\\n\")\n",
    "        print(f\"a1 = sigmoid(z1): \\n{self.a1}\\n\")\n",
    "\n",
    "        # Layer 2\n",
    "        self.z2 = self.W2 @ self.a1 + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        print(f\"z2 = W2 @ a1: \\n{self.z2}\\n\")\n",
    "        print(f\"a2 = sigmoid(z2): \\n{self.a2}\\n\")\n",
    "        print(f\"Predicted y_hat = a2: \\n{self.a2[0, 0]}\\n\")\n",
    "\n",
    "        return self.a2\n",
    "\n",
    "    def calculate_cost(self, y):\n",
    "        if self.a2 is None:\n",
    "            print(\"Run Forward pass\")\n",
    "            return None\n",
    "        cost = 0.5 * np.sum((y - self.a2)**2)\n",
    "        print(f\"Cost: \\n{cost}\\n\")\n",
    "        return cost\n",
    "    \n",
    "    def backpropagation(self, y):\n",
    "        if self.a2 is None or self.a1 is None or self.a0 is None or self.z1 is None or self.z2 is None:\n",
    "            print(\"Run Forward pass\")\n",
    "            return None\n",
    "\n",
    "        print(\"--- Backpropagation ---\")\n",
    "        y_hat = self.a2\n",
    "\n",
    "        # 1. Delta^L (L=2)\n",
    "        delta2 = (y - y_hat) * self.sigmoid_derivative(y_hat)\n",
    "        print(f\" delta^L = delta2 = (y - y_hat) * sigmoid_derivative(y_hat): \\n{delta2}\\n\")\n",
    "\n",
    "        # 2. Delta^l (l=1)\n",
    "        # delta^l = [ (W^{l+1})^T @ delta^{l+1} ] .* sigma'(z^l)\n",
    "        delta1 = (self.W2.T @ delta2) * self.sigmoid_derivative(self.a1)\n",
    "        print(f\"delta1 = (W2.T @ delta2) * sigmoid_derivative(a1): \\n{delta1}\\n\")\n",
    "\n",
    "        # 3. Gradient\n",
    "        # dC/dW^l = delta^l @ (a^{l-1})^T\n",
    "        # dC/db^l = delta^l\n",
    "        grad_W2 = delta2 @ self.a1.T\n",
    "        grad_b2 = delta2\n",
    "        grad_W1 = delta1 @ self.a0.T\n",
    "        grad_b1 = delta1\n",
    "\n",
    "        print(f\"Gradient dC/dW2 = delta2 @ a1.T: \\n{grad_W2}\\n\")\n",
    "        print(f\"Gradient dC/db2 = delta2: \\n{grad_b2}\\n\")\n",
    "        print(f\"Gradient dC/dW1 = delta1 @ a0.T: \\n{grad_W1}\\n\")\n",
    "        print(f\"Gradient dC/db1 = delta1: \\n{grad_b1}\\n\")\n",
    "\n",
    "        return grad_W1, grad_b1, grad_W2, grad_b2\n",
    "    \n",
    "    def update_weights(self, grad_W1, grad_b1, grad_W2, grad_b2, eta):\n",
    "        print(f\"Learning rate: {eta}\\n\")\n",
    "\n",
    "        W1_old = self.W1.copy()\n",
    "        W2_old = self.W2.copy()\n",
    "\n",
    "        self.W1 += eta * grad_W1\n",
    "        self.W2 += eta * grad_W2\n",
    "\n",
    "        print(f\"W1_old: \\n{W1_old}\\n\")\n",
    "        print(f\"W1_new = W1_old + eta * grad_W1: \\n{self.W1}\\n\")\n",
    "        print(f\"W2_old: \\n{W2_old}\\n\")\n",
    "        print(f\"W2_new = W2_old + eta * grad_W2: \\n{self.W2}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "a0_input = np.array([[0.35], [0.7]])\n",
    "y_target = 0.5\n",
    "# Init weights\n",
    "W1_init = np.array([[0.2, 0.2],\n",
    "                    [0.3, 0.3]])\n",
    "W2_init = np.array([[0.3, 0.9]])\n",
    "learning_rate = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Forward Pass ---\n",
      "Input a0: [[0.35]\n",
      " [0.7 ]]\n",
      "\n",
      "z1 = W1 @ a0: \n",
      "[[0.21 ]\n",
      " [0.315]]\n",
      "\n",
      "a1 = sigmoid(z1): \n",
      "[[0.55230791]\n",
      " [0.57810523]]\n",
      "\n",
      "z2 = W2 @ a1: \n",
      "[[0.68598708]]\n",
      "\n",
      "a2 = sigmoid(z2): \n",
      "[[0.66507364]]\n",
      "\n",
      "Predicted y_hat = a2: \n",
      "0.6650736395247564\n",
      "\n",
      "Cost: \n",
      "0.013624653232974609\n",
      "\n",
      "--- Backpropagation ---\n",
      " delta^L = delta2 = (y - y_hat) * sigmoid_derivative(y_hat): \n",
      "[[-0.03677027]]\n",
      "\n",
      "delta1 = (W2.T @ delta2) * sigmoid_derivative(a1): \n",
      "[[-0.00272759]\n",
      " [-0.00807143]]\n",
      "\n",
      "Gradient dC/dW2 = delta2 @ a1.T: \n",
      "[[-0.02030851 -0.02125708]]\n",
      "\n",
      "Gradient dC/db2 = delta2: \n",
      "[[-0.03677027]]\n",
      "\n",
      "Gradient dC/dW1 = delta1 @ a0.T: \n",
      "[[-0.00095466 -0.00190931]\n",
      " [-0.002825   -0.00565   ]]\n",
      "\n",
      "Gradient dC/db1 = delta1: \n",
      "[[-0.00272759]\n",
      " [-0.00807143]]\n",
      "\n",
      "Learning rate: 1\n",
      "\n",
      "W1_old: \n",
      "[[0.2 0.2]\n",
      " [0.3 0.3]]\n",
      "\n",
      "W1_new = W1_old + eta * grad_W1: \n",
      "[[0.19904534 0.19809069]\n",
      " [0.297175   0.29435   ]]\n",
      "\n",
      "W2_old: \n",
      "[[0.3 0.9]]\n",
      "\n",
      "W2_new = W2_old + eta * grad_W2: \n",
      "[[0.27969149 0.87874292]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Init\n",
    "network = NeuralNet(W1_init, W2_init)\n",
    "\n",
    "# 2. Forward Pass\n",
    "network.a0 = a0_input\n",
    "y_hat_output = network.forward_pass(a0_input)\n",
    "\n",
    "# 3. Cost\n",
    "initial_cost = network.calculate_cost(y_target)\n",
    "\n",
    "# 4. Backpropagation -> Gradient\n",
    "gradients = network.backpropagation(y_target)\n",
    "\n",
    "# 5. Update Weights\n",
    "if gradients:\n",
    "    grad_W1, grad_b1, grad_W2, grad_b2 = gradients\n",
    "    network.update_weights(grad_W1, grad_b1, grad_W2, grad_b2, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
