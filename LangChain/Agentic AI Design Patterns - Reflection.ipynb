{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection Pattern - Agentic AI Design Patterns 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Mô Hình Reflection (Phản Xạ)\n",
    "### 1. Định nghĩa\n",
    "- **Reflection** (Phản ánh) là một kỹ thuật trong trí tuệ nhân tạo (AI), đặc biệt áp dụng cho các mô hình ngôn ngữ lớn (Large Language Models - LLM), nhằm cho phép mô hình tự đánh giá và cải thiện đầu ra của chính mình. Đây là một quá trình lặp lại, mô phỏng cách con người suy nghĩ, học hỏi và tinh chỉnh công việc.\n",
    "\n",
    "- Là quá trình bao gồm các bước sau:\n",
    "\t- LLM tạo ra phản hồi cho câu hỏi hoặc nhiệm vụ từ người dùng;\n",
    "\t- LLM tự phản ánh để đánh giá chất lượng, độ chính xác và tính đầy đủ của phản hồi ban đầu;\n",
    "\t- Dựa trên phản ánh đó, LLM tinh chỉnh phản hồi để trả về phiên bản tốt hơn;\n",
    "\t- Quá trình này có thể lặp lại nhiều lần để đạt được kết quả mong muốn.\n",
    "\n",
    "<p align=\"center\"><img src=\"./attachments/ReflectionPipe.png\" alt=\"\" width=\"700\"/></p>\n",
    "\n",
    "- Các thành phần:\n",
    "\t- Prompt: đầu vào của mô hình;\n",
    "\t- Generate: model tạo đầu ra dựa vào prompt;\n",
    "\t- Output: đầu ra của mô hình;\n",
    "\t- Reflect: phân tích, kiểm tra và bổ sung để tăng cường chất lượng đầu ra;\n",
    "\t- Reflected Text: đầu ra sau khi trải qua quá trình Reflect;\n",
    "\t- Iterate: qui trình Reflect được lặp đi lặp lại để tinh chỉnh kết quả cuối cùng;\n",
    "\t- Response: đầu ra cuối cùng sau khi trải qua quá trình Reflect.\n",
    "\n",
    "### 2. Tầm Quan Trọng\n",
    "- **Cải thiện chất lượng đầu ra**: Reflection giúp LLM tạo ra câu trả lời chính xác, chi tiết và phù hợp hơn.\n",
    "- **Mô phỏng tư duy con người**: giống cách con người tự đánh giá và cải thiện, làm cho hành vi của AI trở nên trực quan hơn.\n",
    "- **Tăng khả năng thích nghi**: cho phép AI điều chỉnh phản hồi dựa trên bối cảnh hoặc yêu cầu mới."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Triển khai với LangChain\n",
    "### 1. Các bước triển khai\n",
    "- **LangChain** là một framework mạnh mẽ để phát triển ứng dụng sử dụng LLM.\n",
    "\t1. Khởi tạo mô hình: sử dụng GeminiAPI để minh họa;\n",
    "\t2. Định nghĩa prompt: bao gồm\n",
    "\t\t- `initial_prompt`: Để khởi tạo câu trả lời ban đầu của mô hình;\n",
    "\t\t- `reflection_promp`: Để thực hiện quá trình tinh chỉnh câu trả lời trước đó;\n",
    "\t3. Sử dụng RunnableSequence để liên kết quá trình thực hiện.\n",
    "\t4. Thực thi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Minh họa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = input(\"Your Gemini API key: \")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model       = \"gemini-2.0-flash\",\n",
    "    temperature = 0.55,\n",
    "    max_tokens  = None,\n",
    "    timeout     = None,\n",
    "    max_retries = 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROMPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Prompt\n",
    "initial_prompt = PromptTemplate(\n",
    "    input_variables = [\"question\"],\n",
    "    template        = \"Answer the following question: {question}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reflection Prompt\n",
    "reflection_prompt = PromptTemplate(\n",
    "    input_variables=[\"initial_answer\"],\n",
    "    template=\"Review and improve the following answer to better address the question. Rewrite a detailed final answer, ignore unecessary information: {initial_answer}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PIPELINE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Chain\n",
    "initial_chain = RunnableSequence(initial_prompt | llm)\n",
    "\n",
    "#Reflect Chain\n",
    "reflection_chain = RunnableSequence(reflection_prompt | llm)\n",
    "\n",
    "#PIPELINE\n",
    "def run_reflection_pipeline(input_dict):\n",
    "    try:\n",
    "        #Step 1: Initial Response\n",
    "        logger.info(\"Generating initial answer...\")\n",
    "        initial_answer = initial_chain.invoke({\"question\": input_dict[\"question\"]})\n",
    "        logger.info(\"Initial answer generated.\")\n",
    "\n",
    "        #Step 2: Apply reflection\n",
    "        logger.info(\"Improving answer through reflection...\")\n",
    "        final_answer = reflection_chain.invoke({\"initial_answer\": initial_answer})\n",
    "        logger.info(\"Final answer generated.\")\n",
    "        \n",
    "        return {\"text\": final_answer}\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in reflection pipeline: %s\", e)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating initial answer...\n",
      "INFO:__main__:Initial answer generated.\n",
      "INFO:__main__:Improving answer through reflection...\n",
      "INFO:__main__:Final answer generated.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here's a revised and more direct answer:\n",
       "\n",
       "You must select a fruit from the box labeled \"Apples and Oranges\".  Since all the boxes are mislabeled, the box marked \"Apples and Oranges\" contains either only apples or only oranges.\n",
       "\n",
       "*   **If you pick an apple:**  The box you selected *must* contain only apples.  Because the box labeled \"Oranges\" is also mislabeled, it cannot contain only oranges. Therefore, the box labeled \"Oranges\" must contain the mixed fruit (apples and oranges). This leaves the box labeled \"Apples\" to contain only oranges.\n",
       "\n",
       "*   **If you pick an orange:** The box you selected *must* contain only oranges. Because the box labeled \"Apples\" is mislabeled, it cannot contain only apples. Therefore, the box labeled \"Apples\" must contain the mixed fruit (apples and oranges). This leaves the box labeled \"Oranges\" to contain only apples.\n",
       "\n",
       "By picking a single fruit from the box labeled \"Apples and Oranges\", you immediately determine its true contents, allowing you to deduce the contents of the remaining two boxes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"There are three boxes. One box contains only apples, one box contains only oranges, and one box contains both apples and oranges. All the boxes are labeled incorrectly. You are only allowed to take one fruit from any one box. Which box should you take a fruit from to determine the correct labels of all three boxes?\"\n",
    "result = run_reflection_pipeline({\"question\": question})\n",
    "\n",
    "content = result[\"text\"].content\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
